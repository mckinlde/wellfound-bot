I started a new chat so I can upload files.  Read UHC_pdf_grabber to catch up.  Here's the html of the first page of results.  The script is not finding the pdf links on parsing.  Here's the console output:
```

(.venv) PS C:\Users\mckin\OneDrive\Desktop\syncthing-folder\Git Repos\wellfound-bot> python medicare/google_them/pdf_grabber.py --input medicare/plan_links.csv --output medicare/google_them/testrun/cigna_pdfs.csv --outdir medicare/google_them/testrun --start 1                
Could not find platform independent libraries <prefix>
[INFO] (1/64558) Searching PDFs for H4513-077-3 Cigna Preferred Medicare (HMO)
[SEARCH] H4513-077-3 broad, page 1 → https://www.google.com/search?q=H4513-077-3+Cigna+Preferred+Medicare+%28HMO%29+filetype%3Apdf&hl=en&num=10&start=0
[ACTION] CAPTCHA detected. Solve it in the browser, then press Enter here to continue...
[DEBUG] Re-parsed broad page 1 after captcha solve
[PARSE] broad page 1: 0 PDF links scanned, 0 categorized
[SEARCH] H4513-077-3 broad, page 2 → https://www.google.com/search?q=H4513-077-3+Cigna+Preferred+Medicare+%28HMO%29+filetype%3Apdf&hl=en&num=10&start=10
```


And the full script:
```
import os
import csv
import time
import random
import argparse
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, quote_plus

from utils.driver_session import start_driver

# Selenium waits to ensure we parse search results (not the captcha page)
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

LOG_FILE = "google_pdf_grabber.log"
logging.basicConfig(
    filename=LOG_FILE,
    filemode="a",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("google_pdf_grabber")

DOC_TYPES = {
    "Summary_of_Benefits": "summary of benefits",
    "Evidence_of_Coverage": "evidence of coverage",
    "Drug_Formulary": "formulary drug list",
}

CSV_FIELD_MAP = {
    "Summary_of_Benefits": ("SOB_pdf_link", "SOB_pdf_filepath"),
    "Evidence_of_Coverage": ("EoC_pdf_link", "EoC_pdf_filepath"),
    "Drug_Formulary": ("formulary_pdf_link", "formulary_pdf_filepath"),
}

RESULTS_SELECTORS = "div#search, div#rso"  # used to confirm we're on results page


# ---------------------------
# Utilities
# ---------------------------
def polite_sleep():
    time.sleep(random.uniform(1.0, 2.0))


def wait_for_results_dom(driver, timeout=25):
    """Block until a Google results container appears (after CAPTCHA), then settle briefly."""
    try:
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, RESULTS_SELECTORS))
        )
        time.sleep(1.5)  # settle a bit more
        return True
    except Exception as e:
        print(f"[WARN] Results DOM did not appear after CAPTCHA: {e}")
        logger.warning(f"Results DOM did not appear after CAPTCHA: {e}")
        return False


def save_html(html_dir, plan_id, label, page, html, suffix=""):
    name = f"{plan_id}_{label}_page{page}{suffix}.html"
    path = os.path.join(html_dir, name)
    with open(path, "w", encoding="utf-8") as f:
        f.write(html)
    return path


def extract_google_links_with_text(soup):
    """
    Yield (real_url, anchor_text) pairs from Google result links.
    We only look at /url?q=... wrappers (not cache/images/etc.).
    """
    for a in soup.select("a[href]"):
        href = a.get("href", "")
        if href.startswith("/url?"):
            qs = parse_qs(urlparse(href).query)
            real = qs.get("q", [""])[0]
            if real:
                text = a.get_text(" ", strip=True) or ""
                yield real, text


def is_pdf_url(u: str) -> bool:
    return ".pdf" in u.lower()


def categorize_link(url: str, text: str):
    """
    Categorize based on URL+text. Keep this forgiving and tuned to the Cigna examples you shared:
      - SoB: "summary of benefits", "sob", "sb-"
      - EoC: "evidence of coverage", "eoc"
      - Formulary: "formulary", "drug list", "comprehensive drug list", "part d", "mapd"
    """
    t = f"{url} {text}".lower()

    # Summary of Benefits
    if ("summary of benefits" in t) or (" sob" in t) or ("sob " in t) or (" sb-" in t) or ("-sb" in t):
        return "Summary_of_Benefits"

    # Evidence of Coverage
    if ("evidence of coverage" in t) or (" eoc" in t) or ("eoc " in t) or ("-eoc" in t):
        return "Evidence_of_Coverage"

    # Formulary / Drug list
    if (
        "formulary" in t
        or "drug list" in t
        or "comprehensive drug list" in t
        or "part d" in t
        or "mapd" in t
    ):
        return "Drug_Formulary"

    return None


# ---------------------------
# Search & Parse
# ---------------------------
def parse_and_categorize(driver, plan_id, query, html_dir, label="broad", page=1):
    """
    Run a Google search for one page, save HTML, and return categorized PDF links.
    Critically: if CAPTCHA is present, we pause, then re-parse this SAME page after solve.
    """
    url = f"https://www.google.com/search?q={quote_plus(query)}&hl=en&num=10&start={(page-1)*10}"
    logger.info(f"[SEARCH] {plan_id} {label}, page {page} → {url}")
    print(f"[SEARCH] {plan_id} {label}, page {page} → {url}")

    driver.get(url)
    polite_sleep()

    # Save initial HTML (which might be the captcha page)
    save_html(html_dir, plan_id, label, page, driver.page_source)
    soup = BeautifulSoup(driver.page_source, "html.parser")

    # CAPTCHA detection
    if soup.select_one("#recaptcha-anchor") or "recaptcha" in driver.page_source.lower() or "unusual traffic" in driver.page_source.lower():
        input("[ACTION] CAPTCHA detected. Solve it in the browser, then press Enter here to continue...")
        polite_sleep()
        # Ensure Google has swapped in the actual search results on THIS SAME PAGE
        wait_for_results_dom(driver, timeout=30)
        # Re-save and re-parse after solve
        save_html(html_dir, plan_id, label, page, driver.page_source, suffix="__postcaptcha")
        soup = BeautifulSoup(driver.page_source, "html.parser")
        logger.info(f"[DEBUG] Re-parsed {label} page {page} after captcha solve")
        print(f"[DEBUG] Re-parsed {label} page {page} after captcha solve")

    # Parse for PDFs and categorize
    found = {}
    candidates = []
    for real_url, anchor_text in extract_google_links_with_text(soup):
        if not is_pdf_url(real_url):
            continue
        candidates.append((real_url, anchor_text))

        doc_label = categorize_link(real_url, anchor_text)
        if doc_label and doc_label not in found:
            found[doc_label] = real_url
            logger.info(f"[CANDIDATE] {doc_label} → {real_url}")
            print(f"[CANDIDATE] {doc_label} → {real_url}")

        if len(found) == 3:
            break

    logger.info(f"[PARSE] {label} page {page}: {len(candidates)} PDF links scanned, {len(found)} categorized")
    print(f"[PARSE] {label} page {page}: {len(candidates)} PDF links scanned, {len(found)} categorized")
    return found


def broad_search(driver, plan_id, plan_name, html_dir, max_pages=3):
    """Broad search only: return dict of found docs."""
    results = {}
    query = f"{plan_id} {plan_name} filetype:pdf"
    for page in range(1, max_pages + 1):
        found = parse_and_categorize(driver, plan_id, query, html_dir, label="broad", page=page)
        for k, v in found.items():
            results.setdefault(k, v)
        if len(results) == 3:
            break

    found_labels = list(results.keys())
    missing_labels = [d for d in DOC_TYPES.keys() if d not in results]
    human_found = ", ".join(found_labels) if found_labels else "none"
    human_missing = ", ".join(missing_labels) if missing_labels else "none"
    logger.info(f"[INFO] Broad search found {len(found_labels)}/3 docs ({human_found}; missing: {human_missing})")
    print(f"[INFO] Broad search found {len(found_labels)}/3 docs ({human_found}; missing: {human_missing})")

    return results


def targeted_search(driver, plan_id, plan_name, html_dir, doc_label, max_pages=3):
    """Search specifically for one missing doc label."""
    query = f"{plan_id} {plan_name} {DOC_TYPES[doc_label]} filetype:pdf"
    for page in range(1, max_pages + 1):
        found = parse_and_categorize(driver, plan_id, query, html_dir, label=doc_label, page=page)
        if doc_label in found:
            logger.info(f"[INFO] Targeted search for {doc_label} succeeded")
            print(f"[INFO] Targeted search for {doc_label} succeeded")
            return found[doc_label]
    logger.warning(f"[WARN] Targeted search for {doc_label} failed")
    print(f"[WARN] Targeted search for {doc_label} failed")
    return ""


# ---------------------------
# Download
# ---------------------------
def download_pdf(session, url, dest_path, plan_id, doc_label):
    """Download PDF if not already saved; log destination."""
    if not url:
        return False
    if os.path.exists(dest_path):
        logger.info(f"⏩ Skipping existing {dest_path}")
        print(f"[SKIP] {doc_label} already exists for {plan_id} → {dest_path}")
        return True
    try:
        r = session.get(url, stream=True, timeout=30)
        r.raise_for_status()
        with open(dest_path, "wb") as f:
            for chunk in r.iter_content(8192):
                f.write(chunk)
        logger.info(f"[DOWNLOAD] Saved {doc_label} for {plan_id} → {dest_path}")
        print(f"[DOWNLOAD] Saved {doc_label} for {plan_id} → {dest_path}")
        return True
    except Exception as e:
        logger.error(f"❌ Failed {url}: {e}")
        print(f"[ERROR] Failed {url}: {e}")
        return False


# ---------------------------
# Main
# ---------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--outdir", required=True)
    ap.add_argument("--start", type=int, default=1)
    ap.add_argument("--stop", type=int, default=None, help="Inclusive 1-based stop index")
    ap.add_argument("--pages", type=int, default=3, help="Max Google pages per query (default: 3)")
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)
    html_dir = os.path.join(args.outdir, "html")
    os.makedirs(html_dir, exist_ok=True)

    with open(args.input, newline="", encoding="utf-8") as f:
        reader = list(csv.DictReader(f))
    total = len(reader)

    # normalize slice
    start_idx = max(args.start, 1)
    stop_idx = args.stop if args.stop is not None else total

    out_exists = os.path.exists(args.output)
    out_fh = open(args.output, "a", newline="", encoding="utf-8")
    out_writer = csv.DictWriter(
        out_fh,
        fieldnames=[
            "plan_id","plan_name","company",
            "SOB_pdf_link","SOB_pdf_filepath",
            "EoC_pdf_link","EoC_pdf_filepath",
            "formulary_pdf_link","formulary_pdf_filepath",
        ],
    )
    if not out_exists:
        out_writer.writeheader()

    with start_driver(headless=False) as driver:
        session = requests.Session()

        for idx, plan in enumerate(reader, start=1):
            if idx < start_idx or idx > stop_idx:
                continue

            plan_id = (plan.get("plan_id") or "").strip()
            plan_name = (plan.get("plan_name") or "").strip()
            company = (plan.get("company") or "").strip()

            row = {
                "plan_id": plan_id,
                "plan_name": plan_name,
                "company": company,
                "SOB_pdf_link": "",
                "SOB_pdf_filepath": "",
                "EoC_pdf_link": "",
                "EoC_pdf_filepath": "",
                "formulary_pdf_link": "",
                "formulary_pdf_filepath": "",
            }

            logger.info(f"[INFO] ({idx}/{total}) Searching PDFs for {plan_id} {plan_name}")
            print(f"[INFO] ({idx}/{total}) Searching PDFs for {plan_id} {plan_name}")

            # 1) Broad search → download immediately
            broad_found = broad_search(driver, plan_id, plan_name, html_dir, max_pages=args.pages)

            for doc_label, pdf_url in broad_found.items():
                dest_path = os.path.join(args.outdir, f"{plan_id}_{doc_label}.pdf")
                ok = download_pdf(session, pdf_url, dest_path, plan_id, doc_label)
                if ok:
                    link_field, path_field = CSV_FIELD_MAP[doc_label]
                    row[link_field] = pdf_url
                    row[path_field] = dest_path

            # 2) Targeted search for missing → download
            missing_labels = [d for d in DOC_TYPES.keys() if d not in broad_found]
            for doc_label in missing_labels:
                pdf_url = targeted_search(driver, plan_id, plan_name, html_dir, doc_label, max_pages=args.pages)
                if pdf_url:
                    dest_path = os.path.join(args.outdir, f"{plan_id}_{doc_label}.pdf")
                    ok = download_pdf(session, pdf_url, dest_path, plan_id, doc_label)
                    if ok:
                        link_field, path_field = CSV_FIELD_MAP[doc_label]
                        row[link_field] = pdf_url
                        row[path_field] = dest_path

            # 3) Summary
            summary_bits = [
                "SoB=" + ("FOUND" if row["SOB_pdf_link"] else "NOT FOUND"),
                "EoC=" + ("FOUND" if row["EoC_pdf_link"] else "NOT FOUND"),
                "Formulary=" + ("FOUND" if row["formulary_pdf_link"] else "NOT FOUND"),
            ]
            summary = ", ".join(summary_bits)
            logger.info(f"[SUMMARY] ({idx}/{total}) {plan_id} → {summary}")
            print(f"[SUMMARY] ({idx}/{total}) {plan_id} → {summary}")

            out_writer.writerow(row)
            out_fh.flush()

    out_fh.close()


if __name__ == "__main__":
    main()
# Note: It's generally better to solve captchas with user input and script resuming afterwards.  Horizontal scaling probably involves multiplexing to many machines for captcha solving.
```