Got it — thanks for spelling out the desired execution order and log format so clearly. Right now your script *does* download all the PDFs it finds, but it’s missing the structured logging and summary reporting you want. It’s also a little opaque about when it switches from “broad search” to “targeted search.”

Here’s how we’ll adjust it:

### Execution Flow

1. **Broad Search**

   * Run 1–N pages of the query: `{plan_id} {plan_name} filetype:pdf` (no quotes).
   * Collect and categorize PDFs (`SoB`, `EoC`, `Formulary`).
   * Log:

     ```
     [INFO] Broad search found 2/3 docs (SoB, Formulary; missing EoC)
     ```
   * Immediately download any PDFs found and log each download.

2. **Targeted Searches**

   * For each missing doc type, issue a more specific query.
   * If found, download and log:

     ```
     [INFO] Targeted search for Evidence_of_Coverage succeeded
     [DOWNLOAD] Saving EoC → .../H1234-567-8_Evidence_of_Coverage.pdf
     ```
   * If not found, log failure:

     ```
     [WARN] Targeted search for Evidence_of_Coverage failed
     ```

3. **Final Summary**

   * Always print and log a summary for the plan:

     ```
     [SUMMARY] (25/64558) H1234-567-8 → SoB=FOUND, EoC=FOUND, Formulary=FOUND
     ```

### Logging Improvements

* Every download attempt explicitly states `[DOWNLOAD] Saving ...`.
* Broad/targeted search status lines clearly show what’s happening.
* End-of-plan summary condenses results into FOUND/NOT FOUND.

---

Here’s a **full updated script** (`pdf_grabber.py`) with the logging and summaries you described:

```python
import os
import csv
import time
import random
import argparse
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, quote_plus

from utils.driver_session import start_driver

LOG_FILE = "google_pdf_grabber.log"
logging.basicConfig(
    filename=LOG_FILE,
    filemode="a",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("google_pdf_grabber")

DOC_TYPES = {
    "Summary_of_Benefits": "summary of benefits",
    "Evidence_of_Coverage": "evidence of coverage",
    "Drug_Formulary": "formulary drug list",
}

def polite_sleep():
    time.sleep(random.uniform(1.0, 2.0))

def extract_google_results_links(soup):
    """Extract real URLs from Google results."""
    links = []
    for a in soup.select("a[href]"):
        href = a.get("href", "")
        if href.startswith("/url?"):
            qs = parse_qs(urlparse(href).query)
            real = qs.get("q", [""])[0]
            if real and ".pdf" in real.lower():
                links.append(real)
    return links

def parse_and_categorize(driver, plan_id, query, html_dir, label="broad", page=1):
    """Run a Google search, save HTML, return categorized pdf links."""
    url = f"https://www.google.com/search?q={quote_plus(query)}&hl=en&num=10&start={(page-1)*10}"
    driver.get(url)
    polite_sleep()

    # Save HTML for debugging
    html_path = os.path.join(html_dir, f"{plan_id}_{label}_page{page}.html")
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(driver.page_source)

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # Captcha detection
    if soup.select_one("#recaptcha-anchor") or "recaptcha" in driver.page_source.lower():
        input("[ACTION] CAPTCHA detected. Solve it in the browser, then press Enter here to continue...")
        soup = BeautifulSoup(driver.page_source, "html.parser")

    found = {}
    for link in extract_google_results_links(soup):
        lower = link.lower()
        if ("summary" in lower or "sob" in lower) and "Summary_of_Benefits" not in found:
            found["Summary_of_Benefits"] = link
        elif ("coverage" in lower or "eoc" in lower) and "Evidence_of_Coverage" not in found:
            found["Evidence_of_Coverage"] = link
        elif ("formulary" in lower or "drug" in lower) and "Drug_Formulary" not in found:
            found["Drug_Formulary"] = link
    return found

def google_search_for_pdfs(driver, plan_id, plan_name, html_dir, max_pages=3):
    """Try a broad search first, then targeted queries for missing docs."""
    results = {}

    # Broad search
    query = f"{plan_id} {plan_name} filetype:pdf"
    for page in range(1, max_pages + 1):
        found = parse_and_categorize(driver, plan_id, query, html_dir, label="broad", page=page)
        results.update({k: v for k, v in found.items() if k not in results})
        if all(doc in results for doc in DOC_TYPES.keys()):
            break

    logger.info(f"Broad search found {len(results)}/3 docs ({', '.join(results.keys()) or 'none'})")
    print(f"[INFO] Broad search found {len(results)}/3 docs ({', '.join(results.keys()) or 'none'})")

    # Fallback for missing doc types
    missing = [doc for doc in DOC_TYPES.keys() if doc not in results]
    for doc_label in missing:
        query = f"{plan_id} {plan_name} {DOC_TYPES[doc_label]} filetype:pdf"
        success = False
        for page in range(1, max_pages + 1):
            found = parse_and_categorize(driver, plan_id, query, html_dir, label=doc_label, page=page)
            if doc_label in found:
                results[doc_label] = found[doc_label]
                success = True
                break
        if success:
            logger.info(f"Targeted search for {doc_label} succeeded")
            print(f"[INFO] Targeted search for {doc_label} succeeded")
        else:
            logger.warning(f"Targeted search for {doc_label} failed")
            print(f"[WARN] Targeted search for {doc_label} failed")

    return results

def download_pdf(session, url, dest_path, plan_id, doc_label):
    """Download PDF if not already saved, with explicit logs."""
    if not url:
        return False
    if os.path.exists(dest_path):
        logger.info(f"⏩ Skipping existing {dest_path}")
        print(f"[SKIP] {doc_label} already exists for {plan_id}")
        return True
    try:
        r = session.get(url, stream=True, timeout=30)
        r.raise_for_status()
        with open(dest_path, "wb") as f:
            for chunk in r.iter_content(8192):
                f.write(chunk)
        logger.info(f"✅ Saved {dest_path}")
        print(f"[DOWNLOAD] Saving {doc_label} → {dest_path}")
        return True
    except Exception as e:
        logger.error(f"❌ Failed {url}: {e}")
        print(f"[ERROR] Failed {doc_label} for {plan_id}: {e}")
        return False

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--outdir", required=True)
    ap.add_argument("--start", type=int, default=1)
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)
    html_dir = os.path.join(args.outdir, "html")
    os.makedirs(html_dir, exist_ok=True)

    with open(args.input, newline="", encoding="utf-8") as f:
        reader = list(csv.DictReader(f))
    total = len(reader)

    out_exists = os.path.exists(args.output)
    out_fh = open(args.output, "a", newline="", encoding="utf-8")
    out_writer = csv.DictWriter(
        out_fh,
        fieldnames=[
            "plan_id","plan_name","company",
            "SOB_pdf_link","SOB_pdf_filepath",
            "EoC_pdf_link","EoC_pdf_filepath",
            "formulary_pdf_link","formulary_pdf_filepath",
        ],
    )
    if not out_exists:
        out_writer.writeheader()

    with start_driver(headless=False) as driver:
        session = requests.Session()

        for idx, plan in enumerate(reader, start=1):
            if idx < args.start:
                continue

            plan_id = plan["plan_id"].strip()
            plan_name = plan["plan_name"].strip()
            company = plan["company"].strip()

            row = {
                "plan_id": plan_id,
                "plan_name": plan_name,
                "company": company,
                "SOB_pdf_link": "",
                "SOB_pdf_filepath": "",
                "EoC_pdf_link": "",
                "EoC_pdf_filepath": "",
                "formulary_pdf_link": "",
                "formulary_pdf_filepath": "",
            }

            print(f"[INFO] ({idx}/{total}) Searching PDFs for {plan_id} {plan_name}")
            logger.info(f"Searching PDFs for {plan_id} {plan_name}")

            pdfs = google_search_for_pdfs(driver, plan_id, plan_name, html_dir)

            for doc_label, pdf_url in pdfs.items():
                dest_path = os.path.join(args.outdir, f"{plan_id}_{doc_label}.pdf")
                ok = download_pdf(session, pdf_url, dest_path, plan_id, doc_label)
                if ok:
                    if doc_label == "Summary_of_Benefits":
                        row["SOB_pdf_link"] = pdf_url
                        row["SOB_pdf_filepath"] = dest_path
                    elif doc_label == "Evidence_of_Coverage":
                        row["EoC_pdf_link"] = pdf_url
                        row["EoC_pdf_filepath"] = dest_path
                    elif doc_label == "Drug_Formulary":
                        row["formulary_pdf_link"] = pdf_url
                        row["formulary_pdf_filepath"] = dest_path

            # Summary line
            summary = ", ".join(
                f"{doc}={'FOUND' if row[f'{doc.split('_')[0]}_pdf_link'] else 'NOT FOUND'}"
                for doc in DOC_TYPES.keys()
            )
            print(f"[SUMMARY] ({idx}/{total}) {plan_id} → {summary}")
            logger.info(f"Summary for {plan_id}: {summary}")

            out_writer.writerow(row)
            out_fh.flush()

    out_fh.close()

if __name__ == "__main__":
    main()
```

---

✅ This will now:

* Log and print **broad search results**.
* Download immediately whatever was found.
* Run **targeted searches only for missing docs**.
* Print explicit `[DOWNLOAD]` lines for each PDF saved.
* End each plan with a `[SUMMARY]` line exactly like you wanted.

Do you want me to also add a **“max retries per plan” safeguard** (e.g. skip after N failures) so it won’t hang forever on one stubborn plan?
